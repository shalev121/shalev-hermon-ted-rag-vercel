# TED Talk RAG Assistant

This repository contains an end-to-end Retrieval-Augmented Generation (RAG) system built over the TED Talks dataset.  
The system retrieves relevant transcript chunks using a vector database and generates answers strictly based on the retrieved context.

---

## Repository Structure

## Repository Structure

├── api/
│ ├── prompt.js # POST /api/prompt – main RAG query endpoint
│ └── stats.js # GET /api/stats – returns RAG hyperparameters
│
├── indexing_and_evaluation.ipynb
│ # Python notebook used for data preprocessing, chunking,
│ # embedding creation, Pinecone upserts, and local evaluation
│
├── package.json
├── package-lock.json
└── README.md

---

## System Overview

### 1. Indexing Phase (Offline, Python)
- Load the TED Talks CSV dataset
- Clean and normalize metadata
- Construct embedding text from metadata and transcripts
- Split text into overlapping chunks
- Generate embeddings
- Upsert all chunks into Pinecone

### 2. Query Phase (Online, Vercel API)
- Embed the user question
- Retrieve the top-K relevant chunks from Pinecone
- Build an augmented prompt using the retrieved context
- Generate an answer using the language model

All answers are generated without external knowledge and rely only on retrieved TED data.

---

## API Endpoints

### POST `/api/prompt`

Used to query the RAG system.

### Input (JSON):
{
  "question": "Your natural language question here"
}

### Output (JSON):
{
  "response": "Final answer generated by the model",
  "context": [
    {
      "talk_id": "123",
      "title": "TED Talk title",
      "chunk": "Retrieved transcript chunk",
      "score": 0.123
    }
  ],
  "Augmented_prompt": {
    "System": "System prompt used to query the model",
    "User": "User prompt including retrieved context"
  }
}

### GET /api/stats
Returns the current configuration of the RAG system.
Output (JSON):
{
  "chunk_size": 1024,
  "overlap_ratio": 0.15,
  "top_k": 12
}

## Models and Tools
Embeddings model: RPRTHPB-text-embedding-3-small
Language model: RPRTHPB-gpt-5-mini
Vector database: Pinecone
Serverless deployment: Vercel

## Notebook
The file indexing_and_evaluation.ipynb documents the offline pipeline used before deployment, including:
Dataset preprocessing
Chunking strategy and overlap selection
Embedding generation
Pinecone index creation and upserts
Retrieval and answer quality evaluation
This notebook is provided for transparency and reproducibility.
